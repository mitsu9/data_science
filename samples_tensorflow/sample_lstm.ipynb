{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2\r\n"
     ]
    }
   ],
   "source": [
    "# LSTMのサンプル\n",
    "#  https://www.tensorflow.org/tutorials/sequences/recurrent\n",
    "\n",
    "# 実行環境の出力\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "import collections\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを読み込むためのメソッド群\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "    \n",
    "    # counter.items() = [('voc', cnt), ...]\n",
    "    # key=lambda x: (-x[1], x[0])とすることでcntの降順かつcntが同じときは単語の昇順になる\n",
    "    # count_pairs = [('the', 50770), ('<unk>', 45020), ('<eos>', 42068), ... ]\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    \n",
    "    return word_to_id\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "# データの読み込み\n",
    "data_path = '/tf/notebooks/data/ptb'\n",
    "train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "word_to_id = build_vocab(train_path)\n",
    "train_data = file_to_word_ids(train_path, word_to_id)\n",
    "valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "test_data = file_to_word_ids(test_path, word_to_id)\n",
    "vocabulary = len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↑で作成したdataを実際に使うときのためのメソッドを定義\n",
    "def ptb_producer(raw_data, batch_size, num_steps, name=None):\n",
    "    \"\"\"\n",
    "    raw_dataをtensorに変換して、batch_sizeとnum_stepsからわかる該当のデータを返す\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "        raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "        \n",
    "        data_len = tf.size(raw_data)\n",
    "        batch_len = data_len // batch_size\n",
    "        data = tf.reshape(raw_data[0 : batch_size * batch_len], [batch_size, batch_len])\n",
    "        \n",
    "        epoch_size = (batch_len - 1) // num_steps\n",
    "        assertion = tf.assert_positive(epoch_size, message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "        \n",
    "        with tf.control_dependencies([assertion]):\n",
    "            epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "            \n",
    "        i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "        x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps])\n",
    "        x.set_shape([batch_size, num_steps])\n",
    "        y = tf.strided_slice(data, [0, i * num_steps + 1], [batch_size, (i + 1) * num_steps + 1])\n",
    "        y.set_shape([batch_size, num_steps])\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習のパラメータ\n",
    "class Config(object):\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = \"block\"\n",
    "    \n",
    "config = Config()\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力用のクラス\n",
    "class PTBInput(object):\n",
    "    def __init__(self, config, data, name=None):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.input_data, self.targets = ptb_producer(data, batch_size, num_steps, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習モデルを定義したクラス\n",
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, config, input_):\n",
    "        self._is_training = is_training\n",
    "        self._input = input_\n",
    "        self._rnn_params = None\n",
    "        self._cell = None\n",
    "        self.batch_size = input_.batch_size\n",
    "        self.num_steps = input_.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        # Networkの定義\n",
    "        # embedding -> rnn(lstm) -> softmax\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size], dtype=tf.float32)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
    "            \n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "        \n",
    "        output, state = self._build_rnn_graph_lstm(inputs, config, is_training)\n",
    "        \n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size], dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        \n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            input_.targets,\n",
    "            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True\n",
    "        )\n",
    "        \n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "        \n",
    "        if not is_training:\n",
    "            return\n",
    "        \n",
    "        # training時は重みの更新を行う\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), config.max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "        self._new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "        \n",
    "        \n",
    "    def _build_rnn_graph_lstm(self, inputs, config, is_training):\n",
    "        def make_cell():\n",
    "            cell = tf.contrib.rnn.LSTMBlockCell(config.hidden_size, forget_bias=0.0)\n",
    "            if is_training and config.keep_prob < 1:\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=config.keep_prob)\n",
    "            return cell\n",
    "        \n",
    "        cell = tf.contrib.rnn.MultiRNNCell([make_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "        \n",
    "        self._initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "        state = self._initial_state\n",
    "        \n",
    "        # tutorialではコメントアウトされている方で実装してみた\n",
    "        inputs = tf.unstack(inputs, num=self.num_steps, axis=1)\n",
    "        outputs, state = tf.nn.static_rnn(cell, inputs, initial_state=self._initial_state)\n",
    "        \n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n",
    "        return output, state\n",
    "    \n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "    \n",
    "    def export_ops(self, name):\n",
    "        def export_state_tuples(state_tuples, name):\n",
    "            for state_tuple in state_tuples:\n",
    "                tf.add_to_collection(name, state_tuple.c)\n",
    "                tf.add_to_collection(name, state_tuple.h)\n",
    "        \n",
    "        self._name = name\n",
    "        ops = {self._with_prefix(self._name, \"cost\"): self._cost}\n",
    "        if self._is_training:\n",
    "            ops.update(lr=self._lr, new_lr=self._new_lr, lr_update=self._lr_update)\n",
    "        for name, op in ops.items():\n",
    "            tf.add_to_collection(name, op)\n",
    "        self._initial_state_name = self._with_prefix(self._name, \"initial\")\n",
    "        self._final_state_name = self._with_prefix(self._name, \"final\")\n",
    "        export_state_tuples(self._initial_state, self._initial_state_name)\n",
    "        export_state_tuples(self._final_state, self._final_state_name)\n",
    "    \n",
    "    def import_ops(self):\n",
    "        def import_state_tuples(state_tuples, name, num_replicas):\n",
    "            restored = []\n",
    "            for i in range(len(state_tuples) * num_replicas):\n",
    "                c = tf.get_collection_ref(name)[2 * i + 0]\n",
    "                h = tf.get_collection_ref(name)[2 * i + 1]\n",
    "                restored.append(tf.contrib.rnn.LSTMStateTuple(c, h))\n",
    "            return tuple(restored)\n",
    "        \n",
    "        if self._is_training:\n",
    "            self._train_op = tf.get_collection_ref(\"train_op\")[0]\n",
    "            self._lr = tf.get_collection_ref(\"lr\")[0]\n",
    "            self._new_lr = tf.get_collection_ref(\"new_lr\")[0]\n",
    "            self._lr_update = tf.get_collection_ref(\"lr_update\")[0]\n",
    "        self._cost = tf.get_collection_ref(self._with_prefix(self._name, \"cost\"))[0]\n",
    "        num_replicas = 1\n",
    "        self._initial_state = import_state_tuples(self._initial_state, self._initial_state_name, num_replicas)\n",
    "        self._final_state = import_state_tuples(self._final_state, self._final_state_name, num_replicas)\n",
    "    \n",
    "    def _with_prefix(self, prefix, name):\n",
    "            return \"/\".join((prefix, name))\n",
    "        \n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "    \n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "    \n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "    \n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "    \n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "    \n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "    fetches = {\n",
    "        \"cost\": model.cost,\n",
    "        \"final_state\": model.final_state,\n",
    "    }\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "        \n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "            \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals[\"cost\"]\n",
    "        state = vals[\"final_state\"]\n",
    "\n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "\n",
    "        if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" % (step * 1.0 / model.input.epoch_size, np.exp(costs / iters), iters * model.input.batch_size * max(1, FLAGS.num_gpus) / (time.time() - start_time)))\n",
    "                \n",
    "        return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "Epoch: 1 Train Perplexity: 9988.903\n",
      "Epoch: 1 Valid Perplexity: 7444.862\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "Epoch: 2 Train Perplexity: 7374.813\n",
      "Epoch: 2 Valid Perplexity: 4057.475\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "Epoch: 3 Train Perplexity: 4083.982\n",
      "Epoch: 3 Valid Perplexity: 325166.336\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "Epoch: 4 Train Perplexity: 277633.834\n",
      "Epoch: 4 Valid Perplexity: 3322.948\n",
      "Epoch: 5 Learning rate: 0.500\n",
      "Epoch: 5 Train Perplexity: 4257.768\n",
      "Epoch: 5 Valid Perplexity: 3667.908\n",
      "Epoch: 6 Learning rate: 0.250\n",
      "Epoch: 6 Train Perplexity: 3635.160\n",
      "Epoch: 6 Valid Perplexity: 2447.867\n",
      "Epoch: 7 Learning rate: 0.125\n",
      "Epoch: 7 Train Perplexity: 2636.141\n",
      "Epoch: 7 Valid Perplexity: 2392.297\n",
      "Epoch: 8 Learning rate: 0.062\n",
      "Epoch: 8 Train Perplexity: 2257.246\n",
      "Epoch: 8 Valid Perplexity: 1858.075\n",
      "Epoch: 9 Learning rate: 0.031\n",
      "Epoch: 9 Train Perplexity: 1875.584\n",
      "Epoch: 9 Valid Perplexity: 1740.258\n",
      "Epoch: 10 Learning rate: 0.016\n",
      "Epoch: 10 Train Perplexity: 1881.496\n",
      "Epoch: 10 Valid Perplexity: 2031.153\n",
      "Epoch: 11 Learning rate: 0.008\n",
      "Epoch: 11 Train Perplexity: 2036.987\n",
      "Epoch: 11 Valid Perplexity: 1972.259\n",
      "Epoch: 12 Learning rate: 0.004\n",
      "Epoch: 12 Train Perplexity: 1716.641\n",
      "Epoch: 12 Valid Perplexity: 2047.168\n",
      "Epoch: 13 Learning rate: 0.002\n",
      "Epoch: 13 Train Perplexity: 1817.570\n",
      "Epoch: 13 Valid Perplexity: 2184.079\n",
      "Test Perplexity: 3539.719\n"
     ]
    }
   ],
   "source": [
    "# 学習を行う\n",
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "    \n",
    "    with tf.name_scope(\"Train\"):\n",
    "        train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            m = PTBModel(is_training=True, config=config, input_=train_input)\n",
    "        tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "        tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "        \n",
    "    with tf.name_scope(\"Valid\"):\n",
    "        valid_input = PTBInput(config=config, data=valid_data, name=\"ValidInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mvalid = PTBModel(is_training=False, config=config, input_=valid_input)\n",
    "        tf.summary.scalar(\"Validation_Loss\", mvalid.cost)\n",
    "        \n",
    "    with tf.name_scope(\"Test\"):\n",
    "        test_input = PTBInput(config=eval_config, data=test_data, name=\"TestInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mtest = PTBModel(is_training=False, config=eval_config, input_=test_input)\n",
    "            \n",
    "    models = {\"Train\": m, \"Valid\": mvalid, \"Test\": mtest}\n",
    "    for name, model in models.items():\n",
    "        model.export_ops(name)\n",
    "    metagraph = tf.train.export_meta_graph()\n",
    "    soft_placement = False\n",
    "    \n",
    "with tf.Graph().as_default():\n",
    "    tf.train.import_meta_graph(metagraph)\n",
    "    for model in models.values():\n",
    "        model.import_ops()\n",
    "    sv = tf.train.Supervisor(logdir=None)\n",
    "    config_proto = tf.ConfigProto(allow_soft_placement=soft_placement)\n",
    "    with sv.managed_session(config=config_proto) as session:\n",
    "        for i in range(config.max_max_epoch):\n",
    "            lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)\n",
    "            m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "            print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "            train_perplexity = run_epoch(session, m, eval_op=m.train_op, verbose=True)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "            valid_perplexity = run_epoch(session, mvalid)\n",
    "            print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "        test_perplexity = run_epoch(session, mtest)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
